// AI Service Layer - Handles LLM API calls with Robust Mock/Offline Fallbacks
import type { LLMConfig, StudentState, GameEvent, EventChoice, GameDate, Course } from '../types';

// Generate unique ID
const generateId = () => Math.random().toString(36).substring(2, 9);

const withTimeout = (promise: Promise<any>, ms: number) => {
    return Promise.race([
        promise,
        new Promise((_, reject) => setTimeout(() => reject(new Error(`LLM API Timeout (${ms / 1000}s)`)), ms))
    ]);
};

// Mock Events for Offline Mode
const MOCK_EVENTS = [
    {
        title: "å›¾ä¹¦é¦†çš„å®é™",
        description: "å‘¨æœ«ä½ é€‰æ‹©åœ¨å­¦æ ¡çš„å›¾ä¹¦é¦†åº¦è¿‡ã€‚é˜³å…‰æ´’åœ¨ä¹¦é¡µä¸Šï¼Œå‘¨å›´åªæœ‰ç¿»ä¹¦çš„æ²™æ²™å£°ã€‚ä½ æ„Ÿè§‰è‡ªå·±æ²‰æµ¸åœ¨çŸ¥è¯†çš„æµ·æ´‹ä¸­ã€‚",
        choices: [
            { text: "ä¸“æ³¨å¤ä¹ ä¸“ä¸šè¯¾", effects: { iq: 2, stamina: -5, gpa: 0.1 } },
            { text: "é˜…è¯»ä¸€äº›è¯¾å¤–è¯»ç‰©", effects: { eq: 1, stamina: 5 } }
        ]
    },
    {
        title: "é£Ÿå ‚çš„æ–°èœå¼",
        description: "é£Ÿå ‚äºŒæ¥¼ä»Šå¤©æ¨å‡ºäº†'ç‰¹è‰²åˆ›æ–°æ–™ç†'ã€‚è™½ç„¶çœ‹èµ·æ¥é¢œè‰²æœ‰ç‚¹å¥‡æ€ªï¼Œä½†é—»èµ·æ¥å±…ç„¶è¿˜ä¸é”™ã€‚ä½ çš„è‚šå­é¥¿å¾—å’•å’•å«ã€‚",
        choices: [
            { text: "å‹‡æ•¢å°è¯•æ–°å£å‘³", effects: { stamina: 10, luck: 2 } },
            { text: "è¿˜æ˜¯åƒå›è€ä¸‰æ ·", effects: { stamina: 5, money: -15 } }
        ]
    },
    {
        title: "å®¿èˆæ¥¼ä¸‹çš„å¶é‡",
        description: "æ™šè¯¾å›æ¥ï¼Œä½ åœ¨å®¿èˆæ¥¼ä¸‹å¶é‡äº†å¾ˆä¹…æ²¡è§çš„åŒå­¦ã€‚taçœ‹èµ·æ¥ä¼¼ä¹æœ‰äº›å¿ƒäº‹ï¼Œæ­£ä¸€ä¸ªäººååœ¨é•¿æ¤…ä¸Šå‘å‘†ã€‚",
        choices: [
            { text: "ä¸»åŠ¨ä¸Šå‰æ‰“æ‹›å‘¼å™æ—§", effects: { eq: 2, charm: 1 } },
            { text: "å½“ä½œæ²¡çœ‹è§ï¼Œå¾„ç›´ä¸Šæ¥¼", effects: { stress: -5 } }
        ]
    },
    {
        title: "çªå¦‚å…¶æ¥çš„é˜µé›¨",
        description: "ä»æ•™å­¦æ¥¼å‡ºæ¥ï¼Œå¤©ç©ºçªç„¶ä¸‹èµ·äº†ä¸€é˜µæ€¥ä¿ƒçš„å¤§é›¨ã€‚ä½ æ²¡æœ‰å¸¦ä¼ï¼Œè€Œæ­¤æ—¶æ­£å¥½çœ‹åˆ°ä¸€ä½æ‹¿ç€å¤§ä¼çš„ç†Ÿäººç»è¿‡ã€‚",
        choices: [
            { text: "åšç€è„¸çš®å»è¹­ä¼", effects: { eq: 1, charm: 1, stamina: -2 } },
            { text: "åœ¨å±‹æªä¸‹ç­‰é›¨åœ", effects: { stamina: -5, stress: 5 } },
            { text: "å†’é›¨è·‘å›å®¿èˆ", effects: { stamina: -15, luck: -1 } }
        ]
    },
    {
        title: "æ ¡å›­ç¤¾å›¢æ‹›æ–°",
        description: "è·¯è¿‡å­¦ç”Ÿæ´»åŠ¨ä¸­å¿ƒï¼Œé‚£é‡Œæ­£çƒ­é—¹éå‡¡åœ°è¿›è¡Œç€ç¤¾å›¢æ‹›æ˜Ÿã€‚äº”èŠ±å…«é—¨çš„æ‹›ç‰Œè®©ä½ ç›®ä¸æš‡æ¥ï¼Œå„ç§å­¦é•¿å­¦å§åœ¨çƒ­æƒ…æ‹‰å®¢ã€‚",
        choices: [
            { text: "å¡«è¡¨ç”³è¯·å¿ƒä»ªçš„ç¤¾å›¢", effects: { eq: 2, money: -50 } },
            { text: "åªçœ‹çƒ­é—¹ï¼Œä¸ä¸ºæ‰€åŠ¨", effects: { stamina: 5 } }
        ]
    }
];

// Generate dynamic event based on student state (Synchronous version)
export const generateMockEventSync = (currentDate: GameDate): GameEvent => {
    const mock = MOCK_EVENTS[Math.floor(Math.random() * MOCK_EVENTS.length)];

    const choices: EventChoice[] = mock.choices.map((c, idx) => ({
        id: `choice_${idx}`,
        text: c.text,
        effects: Object.entries(c.effects).map(([key, val]) => {
            if (key === 'gpa') return { type: 'gpa' as const, target: 'gpa', value: val as number };
            if (key === 'money') return { type: 'money' as const, target: 'money', value: val as number };
            return { type: 'attribute' as const, target: key, value: val as number };
        }),
    }));

    return {
        id: `mock_${generateId()}`,
        type: 'dynamic',
        title: mock.title,
        description: mock.description,
        choices,
        isLLMGenerated: true, // We mark as true to satisfy GameEvent type requirements for dynamic events
        timestamp: currentDate,
    };
};

// Generate dynamic event based on student state (Promise wrapper for legacy compatibility)
export const generateMockEvent = (currentDate: GameDate): GameEvent => {
    return generateMockEventSync(currentDate);
};

// Format game state for LLM context
const formatGameContext = (student: StudentState): string => {
    const { attributes, academic, money, npcs, flags, currentDate } = student;
    const semesterStr = currentDate.semester === 1 ? 'ç§‹å­£' : 'æ˜¥å­£';

    return `
Current Student Status:
- Name: ${student.name}, Year ${currentDate.year}, ${semesterStr} Semester, Week ${currentDate.week}
- University: ${academic.universityName} (${academic.universityTier})
- Major: ${academic.major.name}
- GPA: ${academic.gpa.toFixed(2)}
- Money: Â¥${money}
- IQ: ${attributes.iq}, EQ: ${attributes.eq}
- Energy: ${attributes.stamina}%, Stress: ${attributes.stress}%
- Charm: ${attributes.charm}, Luck: ${attributes.luck}
- Dating: ${flags.isDating ? 'Yes' : 'No'}
- Has Job: ${flags.hasJob ? 'Yes' : 'No'}
- Relationships: ${npcs.slice(0, 3).map(n => `${n.name}(${n.role}:${n.relationshipScore})`).join(', ')}
`.trim();
};

// ============ Phase 1: AI Director System ============

/**
 * Token-optimized state summary for AI Director.
 * Only extracts key fields to reduce API costs.
 */
export const summarizeStateForAI = (student: StudentState): string => {
    const { attributes, academic, money, flags, currentDate, npcs, eventHistory } = student;

    // Extract high-intimacy NPCs only
    const topNPCs = npcs
        .filter(n => n.relationshipScore > 30)
        .slice(0, 3)
        .map(n => `${n.name}(${n.role}, å¥½æ„Ÿ${n.relationshipScore})`);

    // Recent events (last 3)
    const recentEvents = eventHistory
        .slice(-3)
        .map(e => e.title)
        .join('; ');

    return JSON.stringify({
        time: { year: currentDate.year, semester: currentDate.semester, week: currentDate.week },
        stats: {
            iq: attributes.iq,
            eq: attributes.eq,
            stamina: attributes.stamina,
            stress: attributes.stress,
            charm: attributes.charm,
            luck: attributes.luck,
        },
        academic: { gpa: academic.gpa.toFixed(2), major: academic.major.name, university: academic.universityName },
        money,
        flags: { isDating: flags.isDating, hasJob: flags.hasJob, hasScholarship: flags.hasScholarship },
        topNPCs,
        recentEvents: recentEvents || 'æ— ',
        weeklyDiary: student.weeklyDiary || null,
    });
};

// Director System Prompt - Strict JSON output
const DIRECTOR_SYSTEM_PROMPT = `ä½ æ˜¯ã€Šå¤§å­¦ç”Ÿæ´»æ¨¡æ‹Ÿå™¨ã€‹çš„ AI å¯¼æ¼” (Game Director)ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç©å®¶å½“å‰çŠ¶æ€ï¼Œå†³å®šè¿™ä¸€å‘¨ä¼šå‘ç”Ÿä»€ä¹ˆæ•…äº‹ã€‚

## è§„åˆ™
1. æ ¹æ®ç©å®¶çš„å‹åŠ›ã€é‡‘é’±ã€å­¦ä¸šçŠ¶æ€ï¼Œç”Ÿæˆåˆç†çš„å™äº‹ã€‚
2. æ ¡å›­æ–°é—»åº”åæ˜ å½“å‰æ—¶é—´èŠ‚ç‚¹ï¼ˆå¼€å­¦ã€è€ƒè¯•å‘¨ã€æ”¾å‡ç­‰ï¼‰ã€‚
3. statChanges æ˜¯å¢é‡å€¼ï¼ˆDeltaï¼‰ï¼Œä¼šå åŠ åˆ°ç©å®¶å±æ€§ä¸Šã€‚
4. åˆç†èŒƒå›´ï¼šstress Â±20, stamina Â±30, money Â±500, gpa Â±0.3

## è¾“å‡ºæ ¼å¼ (Strict JSON)
Output STRICT JSON only. No markdown. No conversational text. Start directly with '{'.
{
  "narrative": "æœ¬å‘¨å‘ç”Ÿçš„æ•…äº‹æè¿° (50-100å­—)",
  "worldNews": "æ ¡å›­/ç¤¾ä¼šæ–°é—»å¤´æ¡ (ä¸€å¥è¯)",
  "statChanges": { "stress": 10, "money": -200, ... },
  "specialEventId": null
}`;

import type { DirectorResponse } from '../types';

const MOCK_DIRECTOR_RESPONSES: DirectorResponse[] = [
    {
        narrative: 'è¿™å‘¨è¿‡å¾—å¹³æ·¡æ— å¥‡ï¼Œæ—©å‡ºæ™šå½’åœ°ä¸Šè¯¾ã€è‡ªä¹ ã€‚é£Ÿå ‚çš„é¥­èœä¸€å¦‚æ—¢å¾€ï¼Œå®¤å‹çš„å‘¼å™œå£°ä¾ç„¶å“äº®ã€‚',
        worldNews: 'å­¦æ ¡å›¾ä¹¦é¦†å°†å»¶é•¿å¼€æ”¾æ—¶é—´è‡³æ™šä¸Š11ç‚¹',
        statChanges: { stamina: -10, stress: 5 },
    },
    {
        narrative: 'å‘¨ä¸‰æ™šä¸Šå’Œå®¤å‹ä¸€èµ·åœ¨å®¿èˆçœ‹äº†åœºç”µå½±ï¼Œéš¾å¾—çš„æ”¾æ¾æ—¶å…‰è®©ä½ æ„Ÿè§‰ç²¾ç¥ç„•å‘ã€‚',
        worldNews: 'åŒåä¸€æ ¡å›­å¿«é€’çˆ†ä»“ï¼Œå–ä»¶éœ€æ’é˜Ÿä¸¤å°æ—¶',
        statChanges: { stress: -15, stamina: 10 },
    },
    {
        narrative: 'è¿™å‘¨ä½ æ²‰è¿·å­¦ä¹ æ— æ³•è‡ªæ‹”ï¼Œè¿ç»­å‡ å¤©æ³¡åœ¨å›¾ä¹¦é¦†ï¼Œæ„Ÿè§‰è„‘å­éƒ½è¦å†’çƒŸäº†ã€‚',
        worldNews: 'å­¦æ ¡é£Ÿå ‚æ¨å‡ºæ–°èœå“ï¼šç¥ç§˜è‚‰ä¸¸å¥—é¤',
        statChanges: { iq: 3, stamina: -20, stress: 15 },
    },
];

/**
 * Generates weekly AI Director plan with narrative and stat changes.
 * Uses "Base Rules + AI Modifiers" pattern - returns Delta values only.
 */
export const generateWeeklyDirectorPlan = async (
    config: LLMConfig,
    student: StudentState
): Promise<DirectorResponse> => {
    // Offline/No API Key fallback
    if (!config.apiKey || config.apiKey.trim() === '') {
        console.log('[AI Director] No API key, using mock response');
        return MOCK_DIRECTOR_RESPONSES[Math.floor(Math.random() * MOCK_DIRECTOR_RESPONSES.length)];
    }

    const stateContext = summarizeStateForAI(student);
    const userPrompt = `å½“å‰ç©å®¶çŠ¶æ€ï¼š\n${stateContext}\n\nè¯·ç”Ÿæˆæœ¬å‘¨çš„æ•…äº‹å’Œå±æ€§å˜åŒ–ã€‚`;

    try {
        const response = await withTimeout(callLLM(config, DIRECTOR_SYSTEM_PROMPT, userPrompt), 20000);

        // Parse JSON response
        let jsonStr = response.trim();
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/);
        if (jsonMatch) jsonStr = jsonMatch[1].trim();

        // Remove potential leading/trailing content
        const jsonStart = jsonStr.indexOf('{');
        const jsonEnd = jsonStr.lastIndexOf('}');
        if (jsonStart !== -1 && jsonEnd !== -1) {
            jsonStr = jsonStr.substring(jsonStart, jsonEnd + 1);
        }

        const data = JSON.parse(jsonStr) as DirectorResponse;

        // Validate and sanitize statChanges
        const sanitized: DirectorResponse = {
            narrative: data.narrative || 'è¿™å‘¨å¹³å®‰æ— äº‹ã€‚',
            worldNews: data.worldNews || 'æ ¡å›­ç”Ÿæ´»ä¸€åˆ‡æ­£å¸¸ã€‚',
            statChanges: {},
            specialEventId: data.specialEventId,
        };

        // Clamp stat changes to reasonable ranges
        if (data.statChanges) {
            const clamp = (val: number | undefined, min: number, max: number) =>
                val !== undefined ? Math.max(min, Math.min(max, val)) : undefined;

            sanitized.statChanges = {
                iq: clamp(data.statChanges.iq, -5, 5),
                eq: clamp(data.statChanges.eq, -5, 5),
                stamina: clamp(data.statChanges.stamina, -30, 30),
                stress: clamp(data.statChanges.stress, -20, 20),
                charm: clamp(data.statChanges.charm, -5, 5),
                luck: clamp(data.statChanges.luck, -5, 5),
                money: clamp(data.statChanges.money, -1000, 1000),
                gpa: clamp(data.statChanges.gpa, -0.3, 0.3),
            };
            // Remove undefined values
            Object.keys(sanitized.statChanges).forEach(key => {
                if ((sanitized.statChanges as any)[key] === undefined) {
                    delete (sanitized.statChanges as any)[key];
                }
            });
        }

        return sanitized;
    } catch (error) {
        console.error('[AI Director] Generation failed, using fallback:', error);
        return MOCK_DIRECTOR_RESPONSES[Math.floor(Math.random() * MOCK_DIRECTOR_RESPONSES.length)];
    }
};

const SYSTEM_PROMPT = `You are the narrator of a Chinese university life simulator game. 
Your role is to generate engaging, realistic, and sometimes humorous campus events.
Response format (JSON only): { "title": "...", "description": "...", "choices": [...] }`;

// Parse LLM response to GameEvent
const parseEventResponse = (
    response: string,
    currentDate: GameDate
): GameEvent | null => {
    try {
        let jsonStr = response;
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/);
        if (jsonMatch) jsonStr = jsonMatch[1];

        // Try to extract JSON from response
        const jsonStart = jsonStr.indexOf('{');
        const jsonEnd = jsonStr.lastIndexOf('}');
        if (jsonStart !== -1 && jsonEnd !== -1) {
            jsonStr = jsonStr.substring(jsonStart, jsonEnd + 1);
        }

        const data = JSON.parse(jsonStr.trim());

        // Handle choices - AI might return different formats
        const rawChoices = data.choices || data.options || [];

        const choices: EventChoice[] = rawChoices.map((c: any, idx: number) => {
            // Try multiple possible field names for choice text
            const choiceText = c.text || c.label || c.option || c.content || c.name || c.choice || '';

            // If choice is a string directly (e.g., ["é€‰é¡¹1", "é€‰é¡¹2"])
            const finalText = typeof c === 'string' ? c : choiceText;

            return {
                id: `choice_${idx}`,
                text: finalText || `é€‰é¡¹ ${idx + 1}`,
                effects: Object.entries(c.effects || {}).map(([key, val]) => {
                    if (key === 'gpa') return { type: 'gpa' as const, target: 'gpa', value: val as number };
                    if (key === 'money') return { type: 'money' as const, target: 'money', value: val as number };
                    return { type: 'attribute' as const, target: key, value: val as number };
                }),
            };
        });

        // Filter out choices with empty text and ensure at least one valid choice
        const validChoices = choices.filter(c => c.text && c.text.trim() !== '');

        if (validChoices.length === 0) {
            console.warn('LLM generated event with no valid choices, falling back to mock.');
            return null;
        }

        return {
            id: generateId(),
            type: 'dynamic',
            title: data.title || 'æ ¡å›­äº‹ä»¶',
            description: data.description || 'å‘ç”Ÿäº†ä¸€äº›äº‹æƒ…...',
            choices: validChoices,
            isLLMGenerated: true,
            timestamp: currentDate,
        };
    } catch (error) {
        console.error('Failed to parse LLM response:', error);
        return null;
    }
};


// Main API call function
export const callLLM = async (
    config: LLMConfig,
    systemPrompt: string,
    userPrompt: string
): Promise<string> => {
    const { provider, apiKey, baseUrl, model, maxTokens, temperature } = config;
    if (!apiKey) throw new Error('API Key is missing');

    let endpoint: string;
    let headers: Record<string, string> = { 'Content-Type': 'application/json' };
    let body: Record<string, unknown>;

    switch (provider) {
        case 'openai':
        case 'custom': // Custom provider uses OpenAI-compatible format
            // Robust base URL handling: trim trailing slashes and ensure /chat/completions is present
            {
                const base = (baseUrl || 'https://api.openai.com/v1').replace(/\/+$/, '');
                endpoint = `${base}/chat/completions`;
                headers['Authorization'] = `Bearer ${apiKey}`;
                body = { model, messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }], max_tokens: maxTokens, temperature };
            }
            break;
        case 'gemini':
            endpoint = baseUrl || `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;
            body = { contents: [{ parts: [{ text: `${systemPrompt}\n\n${userPrompt}` }] }], generationConfig: { maxOutputTokens: maxTokens, temperature } };
            break;
        default:
            // Fallback to OpenAI-compatible format for unknown providers
            {
                const fallbackBase = (baseUrl || 'https://api.openai.com/v1').replace(/\/+$/, '');
                endpoint = `${fallbackBase}/chat/completions`;
                headers['Authorization'] = `Bearer ${apiKey}`;
                body = { model, messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }], max_tokens: maxTokens, temperature };
            }
    }

    const response = await withTimeout(fetch(endpoint, {
        method: 'POST',
        headers,
        body: JSON.stringify(body),
    }), 15000) as Response;

    if (!response.ok) {
        const error = await response.text();
        throw new Error(`LLM API error: ${response.status} - ${error}`);
    }

    const data = await response.json();
    if (provider === 'openai' || provider === 'custom') return data.choices?.[0]?.message?.content || '';
    if (provider === 'gemini') return data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    return data.choices?.[0]?.message?.content || '';
};

// Generate dynamic event with fallback
export const generateDynamicEvent = async (
    config: LLMConfig,
    student: StudentState,
    trigger?: string,
    locationContext?: { name: string; type: string }
): Promise<GameEvent | null> => {
    // 1. HARD SYNC CHECK: If no key, return IMMEDIATELY using Promise.resolve()
    if (!config.apiKey || config.apiKey.trim() === '') {
        console.log("Creating Mock Event (Synchronous Path)");
        return Promise.resolve(generateMockEventSync(student.currentDate));
    }

    const context = formatGameContext(student);

    // NPC Injection
    const randomNpc = student.npcs[Math.floor(Math.random() * student.npcs.length)];
    const npcInjection = randomNpc ? `Involve this NPC if possible: ${randomNpc.name} (${randomNpc.role})` : '';

    let promptTrigger = trigger || '';
    if (locationContext) {
        promptTrigger += `\nCurrent Location: ${locationContext.name} (${locationContext.type}). Generate an event specific to this location.`;
        if (locationContext.type === 'off_campus') promptTrigger += ' (Example: Shopping, Part-time job, City exploration)';
        if (locationContext.type === 'academic') promptTrigger += ' (Example: Study, Research, Competition)';
        if (locationContext.type === 'living') promptTrigger += ' (Example: Relaxing, Socializing, Dorm life)';
    }

    const userPrompt = `${context}\n\n${npcInjection}\n${promptTrigger ? `Trigger/Context: ${promptTrigger}` : 'Generate a random campus event.'}`;

    try {
        const response = await callLLM(config, SYSTEM_PROMPT, userPrompt);
        const event = parseEventResponse(response, student.currentDate);
        return event || generateMockEventSync(student.currentDate);
    } catch (error) {
        console.error('AI Generation Failed, switching to Mock Mode:', error);
        return generateMockEventSync(student.currentDate);
    }
};

// Test LLM connection
export const testLLMConnection = async (config: LLMConfig): Promise<{ success: boolean; error?: string }> => {
    if (!config.apiKey) return { success: false, error: 'è¯·è¾“å…¥ API Key' };

    try {
        const response = await withTimeout(callLLM(
            config,
            'You are a helpful assistant.',
            'Reply with just the word "connected" if you can read this.'
        ), 30000); // 30 second timeout for test
        const success = response.toLowerCase().includes('connected');
        return { success, error: success ? undefined : 'API å“åº”ä¸åŒ¹é…' };
    } catch (error: any) {
        return { success: false, error: error.message || 'è¿æ¥å¤±è´¥' };
    }
};

// ============ Forum LLM Integration ============

const FORUM_SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä¸ªä¸­å›½å¤§å­¦è®ºå›çš„æ¨¡æ‹Ÿå™¨ã€‚æ ¹æ®å­¦ç”Ÿå½“å‰çš„çŠ¶æ€å’Œæ¸¸æˆæ—¶é—´ï¼Œç”Ÿæˆ3-5æ¡è´´è¿‘ç°å®çš„è®ºå›å¸–å­ã€‚
å¸–å­åº”è¯¥åæ˜ æ ¡å›­ç”Ÿæ´»ã€è€ƒè¯•ã€ç¤¾äº¤ç­‰è¯é¢˜ã€‚å¦‚æœæœ‰å³å°†åˆ°æ¥çš„è€ƒè¯•æˆ–äº‹ä»¶ï¼Œè¦ç”Ÿæˆç›¸å…³çš„å¸–å­ã€‚
è¿”å›JSONæ ¼å¼: { "posts": ["å¸–å­1", "å¸–å­2", ...] }`;

const MOCK_FORUM_POSTS = [
    "å¬è¯´äºŒé£Ÿå ‚çš„çº¢çƒ§è‚‰æ¶¨ä»·äº†ï¼ŒçœŸå®åº¦ 80%...",
    "è¿™å‘¨çš„æ•°å­¦å»ºæ¨¡æ¯”èµ›é¢˜ç›®å¤ªå˜æ€äº†å§ï¼",
    "æ±‚é—®ï¼šå“ªä½æ•™æˆçš„æœŸæœ«è€ƒæ¯”è¾ƒå®¹æ˜“è¿‡ï¼Ÿ",
    "å›¾ä¹¦é¦†å åº§å¤§æˆ˜ï¼Œä»Šå¤©åˆå¤±è´¥äº†...",
    "æœ‰æ²¡æœ‰äººä¸€èµ·ç»„é˜Ÿè€ƒç ”ï¼Ÿ",
];

export interface ForumComment {
    id: string;
    author: string;
    content: string;
    timestamp: number;
}

export interface ForumPost {
    id: string;
    title: string;
    content: string;
    author: string;
    likes: number;
    liked: boolean;
    time: string;
    tag: string;
    comments: ForumComment[];
}

export const generateForumPosts = async (
    config: LLMConfig,
    student: StudentState
): Promise<ForumPost[]> => {
    // Generate contextual hints
    const pendingExamNames = student.pendingExams?.map(e => e.name).join(', ') || '';
    const weekInfo = `ç¬¬${student.currentDate.year}å­¦å¹´ ç¬¬${student.currentDate.week}å‘¨`;

    const contextHints = [];
    if (student.currentDate.week >= 16) contextHints.push('æœŸæœ«è€ƒè¯•å‘¨ä¸´è¿‘');
    if (pendingExamNames) contextHints.push(`æœ‰äººæ­£åœ¨å¤‡è€ƒ: ${pendingExamNames}`);
    if (student.currentDate.week === 1) contextHints.push('æ–°å­¦æœŸå¼€å§‹');

    // Randomized author names for forum posts
    const AUTHORS = ['ææ˜', 'ç‹èŠ³', 'å¼ ä¼Ÿ', 'åˆ˜æ´‹', 'é™ˆé™', 'èµµå¼º', 'å­™ä¸½', 'å‘¨æ°', 'å´å¨œ', 'éƒ‘äº‘'];

    // Offline mode fallback
    if (!config.apiKey || config.apiKey.trim() === '') {
        return MOCK_FORUM_POSTS.map((content, i) => ({
            id: `forum_${i}`,
            title: content.substring(0, 20) + '...',
            content,
            author: AUTHORS[Math.floor(Math.random() * AUTHORS.length)],
            likes: Math.floor(Math.random() * 50),
            liked: false,
            time: `${Math.floor(Math.random() * 12) + 1}å°æ—¶å‰`,
            tag: 'æ ¡å›­',
            comments: []
        }));
    }

    try {
        const userPrompt = `å½“å‰æ—¶é—´: ${weekInfo}\nèƒŒæ™¯æç¤º: ${contextHints.join('; ') || 'æ™®é€šæ ¡å›­ç”Ÿæ´»'}\n\nè¯·ç”Ÿæˆ5æ¡è®ºå›å¸–å­ã€‚`;
        const response = await callLLM(config, FORUM_SYSTEM_PROMPT, userPrompt);

        let posts: string[] = [];
        try {
            const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, response];
            const data = JSON.parse(jsonMatch[1]?.trim() || response.trim());
            posts = data.posts || [];
        } catch {
            posts = MOCK_FORUM_POSTS;
        }

        return posts.map((content, i) => {
            const contentStr = typeof content === 'string' ? content : String(content);
            return {
                id: `forum_${Date.now()}_${i}`,
                title: contentStr.substring(0, 20) + '...',
                content: contentStr,
                author: AUTHORS[Math.floor(Math.random() * AUTHORS.length)],
                likes: Math.floor(Math.random() * 50),
                liked: false,
                time: `${Math.floor(Math.random() * 12) + 1}å°æ—¶å‰`,
                tag: 'æ ¡å›­',
                comments: []
            };
        });
    } catch (error) {
        console.error('Forum LLM failed:', error);
        const AUTHORS = ['ææ˜', 'ç‹èŠ³', 'å¼ ä¼Ÿ', 'åˆ˜æ´‹', 'é™ˆé™', 'èµµå¼º', 'å­™ä¸½', 'å‘¨æ°', 'å´å¨œ', 'éƒ‘äº‘'];
        return MOCK_FORUM_POSTS.map((content, i) => ({
            id: `forum_fallback_${i}`,
            title: content.substring(0, 20) + '...',
            content,
            author: AUTHORS[Math.floor(Math.random() * AUTHORS.length)],
            likes: Math.floor(Math.random() * 20),
            liked: false,
            time: `${Math.floor(Math.random() * 12) + 1}å°æ—¶å‰`,
            tag: 'æ ¡å›­',
            comments: []
        }));
    }
};

// ============ Confession Detection System ============

const CONFESSION_DETECTION_PROMPT = `ä½ æ˜¯ä¸€ä¸ªå¯¹è¯åˆ†æåŠ©æ‰‹ã€‚åˆ¤æ–­ç”¨æˆ·çš„æ¶ˆæ¯æ˜¯å¦æ˜¯å‘å¯¹æ–¹è¡¨ç™½/å‘Šç™½/ç¤ºçˆ±ã€‚
è¡¨ç™½çš„ç‰¹å¾åŒ…æ‹¬ï¼šè¡¨è¾¾å–œæ¬¢ã€çˆ±æ„ã€æƒ³åœ¨ä¸€èµ·ã€æƒ³åšç”·/å¥³æœ‹å‹ç­‰ã€‚
åªéœ€å›å¤ "æ˜¯" æˆ– "å¦"ï¼Œä¸è¦è§£é‡Šã€‚`;

// Offline fallback keywords for confession detection
const CONFESSION_KEYWORDS = [
    'å–œæ¬¢ä½ ', 'çˆ±ä½ ', 'æˆ‘å–œæ¬¢', 'æˆ‘çˆ±', 'åšæˆ‘å¥³æœ‹å‹', 'åšæˆ‘ç”·æœ‹å‹',
    'åœ¨ä¸€èµ·', 'äº¤å¾€', 'å‘Šç™½', 'è¡¨ç™½', 'ç‰µæ‰‹', 'çº¦ä¼šå§',
    'æˆ‘çš„å¿ƒ', 'æš—æ‹', 'å¿ƒåŠ¨', 'ä¸€è¾ˆå­', 'å«ç»™æˆ‘', 'å¨¶æˆ‘'
];

/**
 * Detects if a message is a love confession.
 * Uses LLM when available, falls back to keyword matching offline.
 */
export const detectConfession = async (
    config: LLMConfig,
    message: string
): Promise<boolean> => {
    // Offline fallback: keyword matching
    if (!config.apiKey || config.apiKey.trim() === '') {
        return CONFESSION_KEYWORDS.some(keyword => message.includes(keyword));
    }

    try {
        const response = await withTimeout(
            callLLM(config, CONFESSION_DETECTION_PROMPT, `ç”¨æˆ·æ¶ˆæ¯: "${message}"\n\nè¿™æ˜¯è¡¨ç™½å—ï¼Ÿåªå›å¤"æ˜¯"æˆ–"å¦"ã€‚`),
            10000
        );
        const result = response.trim().toLowerCase();
        return result.includes('æ˜¯') || result.includes('yes') || result.includes('true');
    } catch (error) {
        console.error('[Confession Detection] LLM failed, using keyword fallback:', error);
        return CONFESSION_KEYWORDS.some(keyword => message.includes(keyword));
    }
};

// ============ Sentiment Analysis System ============

const SENTIMENT_ANALYSIS_PROMPT = `ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚åˆ†æç”¨æˆ·å‘ç»™æœ‹å‹çš„æ¶ˆæ¯ï¼Œåˆ¤æ–­è¿™æ¡æ¶ˆæ¯å¯¹æ¥æ”¶è€…çš„æ„Ÿå—æ˜¯ï¼š
- positive: å‹å–„ã€å…³å¿ƒã€èµç¾ã€é¼“åŠ±ã€æœ‰è¶£ã€æ¸©æš–çš„æ¶ˆæ¯
- negative: å†’çŠ¯ã€ä¼¤äººã€ä¾®è¾±ã€å†·æ¼ ã€è®½åˆºã€è®©äººéš¾è¿‡çš„æ¶ˆæ¯
- neutral: æ™®é€šé—®å€™ã€æ—¥å¸¸å¯¹è¯ã€ä¿¡æ¯è¯¢é—®

åªå›å¤ä¸€ä¸ªè¯: positive / negative / neutral`;

// Offline fallback keywords
const POSITIVE_KEYWORDS = [
    'è°¢è°¢', 'æ„Ÿè°¢', 'å¼€å¿ƒ', 'é«˜å…´', 'å¤ªæ£’äº†', 'å‰å®³', 'åŠ æ²¹', 'æ”¯æŒä½ ', 'ç›¸ä¿¡ä½ ',
    'å¥½å¯çˆ±', 'çœŸå¥½', 'ä¸é”™', 'ä¼˜ç§€', 'çœŸå‰å®³', 'ä½ æœ€æ£’', 'è¾›è‹¦äº†', 'å¾ˆå¼€å¿ƒ',
    'æƒ³ä½ ', 'æŒ‚å¿µ', 'æ‹…å¿ƒä½ ', 'å…³å¿ƒ', 'ä¿é‡', 'ç…§é¡¾å¥½è‡ªå·±', 'å¾ˆé«˜å…´', 'çœŸå¼€å¿ƒ'
];

const NEGATIVE_KEYWORDS = [
    'è®¨åŒ', 'çƒ¦æ­»', 'æ»š', 'ç™½ç—´', 'è ¢', 'å‚»é€¼', 'å»æ­»', 'æ¶å¿ƒ', 'ä¸‘',
    'æ— èŠ', 'çƒ¦äºº', 'è®¨åŒä½ ', 'åˆ«çƒ¦æˆ‘', 'ä¸æƒ³ç†ä½ ', 'ä¸æƒ³è¯´è¯', 'é—­å˜´',
    'ä½ çœŸå·®', 'æ²¡ç”¨', 'åºŸç‰©', 'åƒåœ¾', 'ä¸é…', 'æ´»è¯¥', 'å¯æ€œ', 'çœŸæƒ¨'
];

export type SentimentType = 'positive' | 'negative' | 'neutral';

export interface SentimentResult {
    sentiment: SentimentType;
    scoreChange: number; // Suggested relationship score change
}

/**
 * Analyzes the sentiment of a chat message.
 * Returns positive/negative/neutral and suggested score change.
 */
export const analyzeSentiment = async (
    config: LLMConfig,
    message: string
): Promise<SentimentResult> => {
    // Offline fallback: keyword matching
    if (!config.apiKey || config.apiKey.trim() === '') {
        if (POSITIVE_KEYWORDS.some(kw => message.includes(kw))) {
            return { sentiment: 'positive', scoreChange: 3 };
        }
        if (NEGATIVE_KEYWORDS.some(kw => message.includes(kw))) {
            return { sentiment: 'negative', scoreChange: -5 };
        }
        return { sentiment: 'neutral', scoreChange: 1 };
    }

    try {
        const response = await withTimeout(
            callLLM(config, SENTIMENT_ANALYSIS_PROMPT, `ç”¨æˆ·æ¶ˆæ¯: "${message}"\n\næƒ…æ„Ÿåˆ†ç±»æ˜¯ï¼Ÿåªå›å¤ positive/negative/neutral å…¶ä¸­ä¹‹ä¸€ã€‚`),
            8000
        );
        const result = response.trim().toLowerCase();

        if (result.includes('positive') || result.includes('æ­£é¢') || result.includes('å‹å–„')) {
            return { sentiment: 'positive', scoreChange: 3 };
        }
        if (result.includes('negative') || result.includes('è´Ÿé¢') || result.includes('ä¼¤äºº')) {
            return { sentiment: 'negative', scoreChange: -5 };
        }
        return { sentiment: 'neutral', scoreChange: 1 };
    } catch (error) {
        console.error('[Sentiment Analysis] LLM failed, using keyword fallback:', error);
        if (POSITIVE_KEYWORDS.some(kw => message.includes(kw))) {
            return { sentiment: 'positive', scoreChange: 3 };
        }
        if (NEGATIVE_KEYWORDS.some(kw => message.includes(kw))) {
            return { sentiment: 'negative', scoreChange: -5 };
        }
        return { sentiment: 'neutral', scoreChange: 1 };
    }
};

// ============ WeChat NPC Chat Integration ============

const NPC_CHAT_SYSTEM_PROMPT = `ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä¸ªä¸­å›½å¤§å­¦ç”Ÿæ´»æ¨¡æ‹Ÿæ¸¸æˆä¸­çš„NPCè§’è‰²ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®NPCçš„æ€§æ ¼å’ŒèƒŒæ™¯ï¼Œç”¨è‡ªç„¶ã€å£è¯­åŒ–çš„ä¸­æ–‡å›å¤ç”¨æˆ·çš„æ¶ˆæ¯ã€‚
å›å¤åº”è¯¥ç®€çŸ­ï¼ˆ1-3å¥è¯ï¼‰ï¼Œç¬¦åˆè§’è‰²è®¾å®šï¼Œå¯ä»¥å¸¦æœ‰è¡¨æƒ…ç¬¦å·ã€‚
ç›´æ¥è¿”å›å›å¤å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•JSONæˆ–é¢å¤–æ ¼å¼ã€‚`;

const GAME_ASSISTANT_PROMPT = `ä½ æ˜¯"å¤§å­¦ç”Ÿæ´»æ¨¡æ‹Ÿå™¨"æ¸¸æˆä¸­çš„AIåŠ©æ‰‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç©å®¶äº†è§£æ¸¸æˆæœºåˆ¶ã€ç»™å‡ºå»ºè®®ã€è§£ç­”é—®é¢˜ã€‚
ä½ äº†è§£æ¸¸æˆçš„æ‰€æœ‰ç³»ç»Ÿï¼šè¡ŒåŠ¨åŠ›(æ¯å‘¨7ç‚¹)ã€å±æ€§(IQ/EQ/ä½“åŠ›/å‹åŠ›/é­…åŠ›/è¿æ°”)ã€è¯ä¹¦è€ƒè¯•ã€å…¼èŒå·¥ä½œç­‰ã€‚
ç”¨å‹å¥½ã€ç®€æ´çš„ä¸­æ–‡å›å¤ï¼Œå¯ä»¥å¸¦è¡¨æƒ…ç¬¦å·ã€‚ç›´æ¥è¿”å›å›å¤å†…å®¹ã€‚`;

// ============ Phase 3: NPC Memory System ============

const MEMORY_CONSOLIDATION_PROMPT = `ä½ æ˜¯ä¸€ä¸ªè®°å¿†åˆ†æåŠ©æ‰‹ã€‚åˆ†æä»¥ä¸‹å¯¹è¯è®°å½•ï¼Œæå–1-2ä¸ªå…³é”®äº‹å®ä¾›NPCé•¿æœŸè®°ä½ã€‚
å…³é”®äº‹å®åº”è¯¥æ˜¯ï¼š
1. ç©å®¶é€éœ²çš„ä¸ªäººä¿¡æ¯ï¼ˆå–œå¥½ã€ä¹ æƒ¯ã€ç›®æ ‡ï¼‰
2. é‡è¦çš„å…±åŒç»å†æˆ–æ‰¿è¯º
3. æƒ…æ„Ÿä¸Šæœ‰æ„ä¹‰çš„äº’åŠ¨

è¾“å‡ºæ ¼å¼ (Strict JSON only, no markdown):
{ "memories": ["è®°å¿†1", "è®°å¿†2"] }

å¦‚æœæ²¡æœ‰å€¼å¾—è®°ä½çš„å†…å®¹ï¼Œè¿”å›ç©ºæ•°ç»„ï¼š{ "memories": [] }`;

/**
 * Phase 3: Consolidates chat history into long-term memories.
 * Triggered when chatHistory.length > 10 (Lazy Consolidation).
 */
export const consolidateMemory = async (
    config: LLMConfig,
    chatHistory: { role: 'user' | 'npc'; content: string }[],
    npcName: string
): Promise<string[]> => {
    // Offline fallback
    if (!config.apiKey || config.apiKey.trim() === '') {
        return [];
    }

    const historyText = chatHistory
        .map(msg => `${msg.role === 'user' ? 'ç©å®¶' : npcName}: ${msg.content}`)
        .join('\n');

    const userPrompt = `å¯¹è¯è®°å½•:\n${historyText}\n\nè¯·æå–å…³é”®è®°å¿†ã€‚`;

    try {
        const response = await withTimeout(callLLM(config, MEMORY_CONSOLIDATION_PROMPT, userPrompt), 15000);

        let jsonStr = response.trim();
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/);
        if (jsonMatch) jsonStr = jsonMatch[1].trim();

        const jsonStart = jsonStr.indexOf('{');
        const jsonEnd = jsonStr.lastIndexOf('}');
        if (jsonStart !== -1 && jsonEnd !== -1) {
            jsonStr = jsonStr.substring(jsonStart, jsonEnd + 1);
        }

        const data = JSON.parse(jsonStr);
        return Array.isArray(data.memories) ? data.memories.slice(0, 3) : [];
    } catch (error) {
        console.error('[Memory Consolidation] Failed:', error);
        return [];
    }
};

/**
 * Generates NPC reply with optional long-term memory injection.
 */
export const generateNPCReply = async (
    config: LLMConfig,
    npc: { name: string; personality: string; role: string; longTermMemories?: string[] },
    userMessage: string,
    chatHistory: { role: 'user' | 'npc'; content: string }[],
    isGameAssistant: boolean = false
): Promise<string> => {
    // Build memory-aware system prompt
    let systemPrompt = isGameAssistant ? GAME_ASSISTANT_PROMPT : NPC_CHAT_SYSTEM_PROMPT;

    // Phase 3: Inject long-term memories into NPC's system prompt
    if (!isGameAssistant && npc.longTermMemories && npc.longTermMemories.length > 0) {
        const memoriesText = npc.longTermMemories.join('ï¼›');
        systemPrompt += `\n\nã€ä½ å¯¹ç©å®¶çš„å°è±¡ã€‘ï¼š${memoriesText}\nè¯·åœ¨å¯¹è¯ä¸­è‡ªç„¶åœ°è¡¨ç°å‡ºè¿™äº›è®°å¿†ï¼Œä½†ä¸è¦åˆ»æ„æåŠã€‚`;
    }

    // Build conversation context
    const historyContext = chatHistory.slice(-6).map(msg =>
        `${msg.role === 'user' ? 'ç©å®¶' : npc.name}: ${msg.content}`
    ).join('\n');

    const userPrompt = isGameAssistant
        ? `${historyContext}\nç©å®¶: ${userMessage}\n\nè¯·å›å¤ç©å®¶çš„é—®é¢˜ã€‚`
        : `NPCä¿¡æ¯:\n- åå­—: ${npc.name}\n- è§’è‰²: ${npc.role}\n- æ€§æ ¼: ${npc.personality}\n\nå¯¹è¯è®°å½•:\n${historyContext}\nç©å®¶: ${userMessage}\n\nè¯·ä»¥${npc.name}çš„èº«ä»½å›å¤ã€‚`;

    // Offline fallback
    if (!config.apiKey || config.apiKey.trim() === '') {
        const fallbacks = isGameAssistant
            ? ['è¿™ä¸ªé—®é¢˜æˆ‘æš‚æ—¶å›ç­”ä¸äº†ï¼Œå»ºè®®ä½ æ¢ç´¢ä¸€ä¸‹æ¸¸æˆï¼ğŸ®', 'è¯•è¯•çœ‹ä¸åŒçš„é€‰æ‹©ï¼Œå¯èƒ½ä¼šæœ‰æƒŠå–œå“¦ï¼âœ¨', 'è®°å¾—ç®¡ç†å¥½ä½ çš„è¡ŒåŠ¨åŠ›å’Œä½“åŠ›ï¼ğŸ’ª']
            : ['å“ˆå“ˆï¼Œä½ è¯´å¾—å¯¹ï¼', 'æœ€è¿‘æ€ä¹ˆæ ·å•Šï¼Ÿ', 'æœ‰ç©ºä¸€èµ·å»é£Ÿå ‚åƒé¥­å§ï¼', 'è€ƒè¯•å¤ä¹ å¾—æ€ä¹ˆæ ·äº†ï¼Ÿ'];
        return fallbacks[Math.floor(Math.random() * fallbacks.length)];
    }

    try {
        const response = await withTimeout(callLLM(config, systemPrompt, userPrompt), 30000); // 30s timeout
        return response.trim() || '...(æ²‰é»˜)';
    } catch (error) {
        console.error('NPC Chat LLM failed:', error);
        return isGameAssistant ? 'æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ï¼Œç¨åå†èŠï¼' : '...(å¯¹æ–¹ä¼¼ä¹åœ¨å¿™)';
    }
};

/**
 * Generates a proactive message from an NPC to the player.
 * Used when an NPC "decides" to text the player first.
 */
export const generateProactiveMessage = async (
    config: LLMConfig,
    npc: { name: string; personality: string; role: string },
    student: StudentState
): Promise<string> => {
    const { year, semester, week } = student.currentDate;
    const timeInfo = `ç¬¬${year}å­¦å¹´, ${semester === 1 ? 'ä¸Šå­¦æœŸ' : 'ä¸‹å­¦æœŸ'}, ç¬¬${week}å‘¨`;

    const systemPrompt = `ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä¸ªä¸­å›½å¤§å­¦ç”Ÿæ´»æ¨¡æ‹Ÿæ¸¸æˆä¸­çš„NPCè§’è‰²ï¼š${npc.name}ã€‚
ä½ çš„è§’è‰²æ˜¯ç©å®¶çš„${npc.role}ï¼Œæ€§æ ¼æ˜¯${npc.personality}ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä¸»åŠ¨ç»™ç©å®¶å‘ä¸€æ¡å¾®ä¿¡æ¶ˆæ¯ã€‚æ¶ˆæ¯åº”è¯¥è‡ªç„¶ã€å£è¯­åŒ–ï¼Œåæ˜ å½“å‰çš„æ ¡å›­ç”Ÿæ´»èƒŒæ™¯ã€‚
èƒŒæ™¯æ—¶é—´ï¼š${timeInfo}ã€‚
å¦‚æœæ˜¯å­¦æœŸåˆï¼Œå¯ä»¥é—®å€™å¼€å­¦ï¼›å¦‚æœæ˜¯å­¦æœŸæœ«ï¼Œå¯ä»¥æè€ƒè¯•æˆ–æ”¾å‡ï¼›å¹³æ—¶å¯ä»¥èŠå…«å¦ã€çº¦é¥­æˆ–åˆ†äº«è¶£äº‹ã€‚
å›å¤åº”è¯¥ç®€çŸ­ï¼ˆ1-2å¥ï¼‰ï¼Œç›´æ¥è¿”å›æ¶ˆæ¯å†…å®¹ã€‚`;

    const userPrompt = `ç”±äºç°åœ¨æ˜¯${timeInfo}ï¼Œè¯·ä»¥${npc.name}çš„èº«ä»½ç»™ç©å®¶å‘ä¸€æ¡å¼€åœºç™½ã€‚`;

    if (!config.apiKey || config.apiKey.trim() === '') {
        const fallbacks = [
            'å˜¿ï¼Œæœ€è¿‘åœ¨å¿™ä»€ä¹ˆå‘¢ï¼Ÿ',
            'ä»Šå¤©é£Ÿå ‚çš„é¥­èœä¸é”™ï¼Œè¦ä¸è¦ä¸€èµ·å»ï¼Ÿ',
            'æ„Ÿè§‰è¿™å‘¨çš„è¯¾å¥½ç´¯å•Šï¼Œä½ å‘¢ï¼Ÿ',
            'åˆšæ‰åœ¨å›¾ä¹¦é¦†çœ‹åˆ°ä½ äº†ï¼Œæ„Ÿè§‰ä½ å­¦å¾—å¥½è®¤çœŸï¼',
            'å‘¨æœ«æœ‰ç©ºå—ï¼Ÿæƒ³çº¦ä½ å‡ºå»ç©ã€‚'
        ];
        return fallbacks[Math.floor(Math.random() * fallbacks.length)];
    }

    try {
        const response = await withTimeout(callLLM(config, systemPrompt, userPrompt), 15000);
        return response.trim() || 'æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ';
    } catch (error) {
        console.error('Proactive message LLM failed:', error);
        return 'æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ';
    }
};

/**
 * Generates a WeChat moment content for an NPC.
 */
export const generateMoment = async (
    config: LLMConfig,
    npc: { name: string; personality: string; role: string },
    gameDate: { year: number; semester: number; week: number }
): Promise<string> => {
    const timeInfo = `ç¬¬${gameDate.year}å­¦å¹´, ${gameDate.semester === 1 ? 'ä¸Šå­¦æœŸ' : 'ä¸‹å­¦æœŸ'}, ç¬¬${gameDate.week}å‘¨`;

    const systemPrompt = `ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä¸ªä¸­å›½å¤§å­¦ç”ŸNPCï¼š${npc.name}ã€‚
è§’è‰²ï¼šç©å®¶çš„${npc.role}ï¼Œæ€§æ ¼ï¼š${npc.personality}ã€‚
ä»»åŠ¡ï¼šå‘ä¸€æ¡å¾®ä¿¡æœ‹å‹åœˆã€‚
è¦æ±‚ï¼š
1. å†…å®¹ç®€çŸ­ï¼ˆ1-3å¥ï¼‰ï¼Œå£è¯­åŒ–ã€‚
2. ç»“åˆå½“å‰æ—¶é—´ï¼ˆ${timeInfo}ï¼‰å’Œæ ¡å›­ç”Ÿæ´»ã€‚
3. å¯ä»¥å¸¦ç‚¹æƒ…ç»ªæˆ–åæ§½ï¼Œæˆ–è€…åˆ†äº«æ—¥å¸¸ç”Ÿæ´»ã€‚
4. ä¸éœ€è¦å¸¦è¯é¢˜æ ‡ç­¾ã€‚
ç›´æ¥è¿”å›æœ‹å‹åœˆæ­£æ–‡å†…å®¹ã€‚`;

    const userPrompt = `è¯·ç”Ÿæˆä¸€æ¡æœ‹å‹åœˆå†…å®¹ã€‚`;

    if (!config.apiKey || config.apiKey.trim() === '') {
        const fallbacks = [
            'ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œé€‚åˆå»å›¾ä¹¦é¦†åˆ·é¢˜ï¼',
            'é£Ÿå ‚çš„éº»è¾£é¦™é”…è¶Šæ¥è¶Šå¥½åƒäº†ï¼Œæ¨èï¼',
            'åˆè¦äº¤ä½œä¸šäº†ï¼Œèµ¶æ­»çº¿ä¸­...',
            'å‘¨æœ«æœ‰æ²¡æœ‰äººä¸€èµ·å»çœ‹ç”µå½±ï¼Ÿ',
            'åˆšè·‘å®Œæ­¥ï¼Œæ„Ÿè§‰æ•´ä¸ªäººéƒ½ç²¾ç¥äº†ã€‚',
            'è¿™å‘¨çš„è¯¾å¥½å¤šå•Šï¼Œæ±‚å®‰æ…°ã€‚'
        ];
        return fallbacks[Math.floor(Math.random() * fallbacks.length)];
    }

    try {
        const response = await withTimeout(callLLM(config, systemPrompt, userPrompt), 15000);
        return response.trim() || 'ä»Šå¤©å¿ƒæƒ…ä¸é”™ï¼';
    } catch (error) {
        console.error('Moment generation LLM failed:', error);
        return 'ä»Šå¤©å¿ƒæƒ…ä¸é”™ï¼';
    }
};
/**
 * Generates a list of courses using LLM based on major and grade.
 */
export const generateLLMCourses = async (
    config: LLMConfig,
    major: { name: string; id: string },
    year: number,
    semester: number,
    count: number
): Promise<Partial<Course>[]> => {
    const semName = semester === 1 ? 'ä¸Šå­¦æœŸ' : 'ä¸‹å­¦æœŸ';
    const yearName = year === 1 ? 'å¤§ä¸€' : year === 2 ? 'å¤§äºŒ' : year === 3 ? 'å¤§ä¸‰' : 'å¤§å››';

    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªå¤§å­¦æ•™åŠ¡ç³»ç»Ÿæ¨¡æ‹Ÿå™¨ã€‚æ ¹æ®ç»™å®šçš„ä¸“ä¸šå’Œå¹´çº§ï¼Œç”Ÿæˆä¸€ç»„ç¬¦åˆé€»è¾‘çš„ç¡¬æ ¸è¯¾ç¨‹ã€‚
è¿”å›JSONæ ¼å¼: { "courses": [ { "name": "...", "credits": 2-4, "type": "Required/Elective", "statBonus": { "iq": 1-3, ... } } ] }
ç”Ÿæˆçš„æ•°é‡åº”ä¸º: ${count}ã€‚
ä¸“ä¸š: ${major.name}
å¹´çº§: ${yearName}${semName}`;

    if (!config.apiKey || config.apiKey.trim() === '') {
        return []; // Caller handles fallback
    }

    try {
        const response = await withTimeout(callLLM(config, systemPrompt, `è¯·ç”Ÿæˆ ${count} é—¨è¯¾ç¨‹ã€‚`), 20000);
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, response];
        const data = JSON.parse(jsonMatch[1]?.trim() || response.trim());
        return data.courses || [];
    } catch (error) {
        console.error('LLM Course generation failed:', error);
        return [];
    }
};

// ============ Game Tasks LLM Generation ============

export interface GameTask {
    id: string;
    title: string;
    description: string;
    type: 'daily' | 'weekly' | 'story';
    priority: 'high' | 'medium' | 'low';
    reward?: string;
}

const MOCK_TASKS: GameTask[] = [
    { id: 'task_1', title: 'å®Œæˆæœ¬å‘¨è¯¾ç¨‹', description: 'å‚åŠ è‡³å°‘3èŠ‚ä¸“ä¸šè¯¾', type: 'weekly', priority: 'high', reward: '+çŸ¥è¯†ç‚¹' },
    { id: 'task_2', title: 'å›¾ä¹¦é¦†è‡ªä¹ ', description: 'å»ä¸€æ¬¡å›¾ä¹¦é¦†å¤ä¹ åŠŸè¯¾', type: 'daily', priority: 'medium', reward: '+IQ' },
    { id: 'task_3', title: 'ç¤¾äº¤æ´»åŠ¨', description: 'ä¸åŒå­¦æˆ–å®¤å‹äº’åŠ¨ä¸€æ¬¡', type: 'daily', priority: 'low', reward: '+EQ' },
];

export const generateGameTasks = async (
    config: LLMConfig,
    student: StudentState
): Promise<GameTask[]> => {
    if (!config.apiKey || config.apiKey.trim() === '') {
        return MOCK_TASKS;
    }

    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªå¤§å­¦ç”Ÿæ´»æ¨¡æ‹Ÿæ¸¸æˆçš„ä»»åŠ¡ç³»ç»Ÿã€‚æ ¹æ®å­¦ç”Ÿå½“å‰çŠ¶æ€ç”Ÿæˆ3-5ä¸ªæ¸¸æˆä»»åŠ¡ã€‚
è¿”å›JSONæ ¼å¼: { "tasks": [ { "title": "...", "description": "...", "type": "daily/weekly/story", "priority": "high/medium/low", "reward": "+å±æ€§æˆ–å¥–åŠ±æè¿°" } ] }
ä»»åŠ¡ç±»å‹: daily(æ—¥å¸¸), weekly(æœ¬å‘¨), story(å‰§æƒ…)
ä»»åŠ¡åº”ç»“åˆå­¦ç”Ÿå½“å‰çŠ¶æ€ã€å­¦å¹´ã€ä¸“ä¸šæ¥ç”Ÿæˆã€‚`;

    const userPrompt = `å­¦ç”Ÿ: ${student.name}, ç¬¬${student.currentDate.year}å¹´ç¬¬${student.currentDate.week}å‘¨
ä¸“ä¸š: ${student.academic.major.name}
GPA: ${student.academic.gpa.toFixed(2)}
å±æ€§: IQ ${student.attributes.iq}, EQ ${student.attributes.eq}, ä½“åŠ› ${student.attributes.stamina}
è¯·ç”Ÿæˆä»»åŠ¡ã€‚`;

    try {
        const response = await withTimeout(callLLM(config, systemPrompt, userPrompt), 15000);
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, response];
        const data = JSON.parse(jsonMatch[1]?.trim() || response.trim());
        return (data.tasks || []).map((t: any) => ({
            id: `task_${generateId()}`,
            title: t.title,
            description: t.description,
            type: t.type || 'daily',
            priority: t.priority || 'medium',
            reward: t.reward
        }));
    } catch (error) {
        console.error('LLM Task generation failed:', error);
        return MOCK_TASKS;
    }
};

// ============ NPC Profile LLM Generation ============

export interface NPCProfile {
    backstory: string;
    hobby: string;
    dream: string;
    secretTrait: string;
    relationshipAdvice: string;
}

const MOCK_PROFILE: NPCProfile = {
    backstory: 'æ¥è‡ªä¸€ä¸ªæ™®é€šå®¶åº­ï¼Œé«˜è€ƒåæ¥åˆ°è¿™æ‰€å¤§å­¦ï¼Œæ¢¦æƒ³ç€èƒ½å¤Ÿæ”¹å˜è‡ªå·±çš„å‘½è¿ã€‚',
    hobby: 'å–œæ¬¢åœ¨é—²æš‡æ—¶é—´çœ‹åŠ¨æ¼«ã€æ‰“æ¸¸æˆï¼Œå¶å°”ä¹Ÿä¼šå»æ“åœºè·‘æ­¥ã€‚',
    dream: 'å¸Œæœ›æ¯•ä¸šåèƒ½æ‰¾åˆ°ä¸€ä»½ç¨³å®šçš„å·¥ä½œï¼Œä¹°æˆ¿ä¹°è½¦ï¼Œè®©çˆ¶æ¯è¿‡ä¸Šå¥½æ—¥å­ã€‚',
    secretTrait: 'è¡¨é¢ä¸Šçœ‹èµ·æ¥å¾ˆå¼€æœ—ï¼Œä½†å…¶å®å†…å¿ƒæœ‰äº›è‡ªå‘ï¼Œå®³æ€•è¢«åˆ«äººçœ‹ä¸èµ·ã€‚',
    relationshipAdvice: 'å¤šå…³å¿ƒtaçš„æƒ…ç»ªå˜åŒ–ï¼Œé€‚å½“é€äº›å°ç¤¼ç‰©å¯ä»¥å¿«é€Ÿæå‡å¥½æ„Ÿåº¦ã€‚'
};

export const generateNPCProfile = async (
    config: LLMConfig,
    npc: { name: string; personality: string; role: string; gender: string },
    student: StudentState
): Promise<NPCProfile> => {
    if (!config.apiKey || config.apiKey.trim() === '') {
        return MOCK_PROFILE;
    }

    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªå¤§å­¦ç”Ÿæ´»æ¨¡æ‹Ÿæ¸¸æˆçš„è§’è‰²æè¿°ç”Ÿæˆå™¨ã€‚ä¸ºç»™å®šçš„NPCç”Ÿæˆè¯¦ç»†çš„ä¸ªäººèµ„æ–™ã€‚
è¿”å›JSONæ ¼å¼: { 
    "backstory": "è§’è‰²èƒŒæ™¯æ•…äº‹(50-100å­—)", 
    "hobby": "å…´è¶£çˆ±å¥½", 
    "dream": "äººç”Ÿæ¢¦æƒ³", 
    "secretTrait": "éšè—ç‰¹è´¨æˆ–ç§˜å¯†",
    "relationshipAdvice": "æ”»ç•¥è¯¥è§’è‰²çš„å»ºè®®"
}
ç”Ÿæˆçš„å†…å®¹åº”ç¬¦åˆä¸­å›½å¤§å­¦ç”Ÿæ´»åœºæ™¯ï¼Œè¯­è¨€ç”ŸåŠ¨æœ‰è¶£ã€‚`;

    const userPrompt = `è§’è‰²: ${npc.name}
æ€§åˆ«: ${npc.gender === 'male' ? 'ç”·' : 'å¥³'}
èº«ä»½: ${npc.role === 'roommate' ? 'å®¤å‹' : npc.role === 'classmate' ? 'åŒå­¦' : npc.role === 'professor' ? 'æ•™æˆ' : npc.role === 'friend' ? 'æœ‹å‹' : 'å…¶ä»–'}
æ€§æ ¼: ${npc.personality}
ç©å®¶: ${student.name} (${student.academic.major.name}ä¸“ä¸š)
è¯·ç”Ÿæˆè¯¥è§’è‰²çš„è¯¦ç»†èµ„æ–™ã€‚`;

    try {
        const response = await withTimeout(callLLM(config, systemPrompt, userPrompt), 15000);
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, response];
        const data = JSON.parse(jsonMatch[1]?.trim() || response.trim());
        return {
            backstory: data.backstory || MOCK_PROFILE.backstory,
            hobby: data.hobby || MOCK_PROFILE.hobby,
            dream: data.dream || MOCK_PROFILE.dream,
            secretTrait: data.secretTrait || MOCK_PROFILE.secretTrait,
            relationshipAdvice: data.relationshipAdvice || MOCK_PROFILE.relationshipAdvice
        };
    } catch (error) {
        console.error('LLM NPC Profile generation failed:', error);
        return MOCK_PROFILE;
    }
};

// ============ Phase 2: Exam System ============

import type { ExamQuestion, ExamResult } from '../types';

const EXAM_SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä½ä¸­å›½å¤§å­¦çš„æ•™æˆï¼Œéœ€è¦ä¸ºå­¦ç”Ÿå‡ºä¸€å¥—è€ƒè¯•é¢˜ã€‚
æ ¹æ®è¯¾ç¨‹åç§°å’Œéš¾åº¦ï¼Œç”Ÿæˆ3é“å•é€‰é¢˜ã€‚
éš¾åº¦1-5ï¼š1=å…¥é—¨ï¼Œ3=ä¸­ç­‰ï¼Œ5=æŒ‘æˆ˜æ€§é«˜

è¾“å‡ºæ ¼å¼ (Strict JSON only, no markdown):
{
  "questions": [
    { "text": "é¢˜ç›®", "options": ["A. é€‰é¡¹1", "B. é€‰é¡¹2", "C. é€‰é¡¹3", "D. é€‰é¡¹4"], "correctIndex": 0 }
  ]
}`;

const MOCK_EXAM_QUESTIONS: ExamQuestion[] = [
    { text: "ä¸‹åˆ—å“ªä¸ªä¸æ˜¯é¢å‘å¯¹è±¡ç¼–ç¨‹çš„ç‰¹å¾ï¼Ÿ", options: ["A. å°è£…", "B. ç»§æ‰¿", "C. å¤šæ€", "D. é€’å½’"], correctIndex: 3 },
    { text: "HTTPåè®®é»˜è®¤ä½¿ç”¨çš„ç«¯å£å·æ˜¯ï¼Ÿ", options: ["A. 21", "B. 22", "C. 80", "D. 443"], correctIndex: 2 },
    { text: "åœ¨æ•°æ®åº“ä¸­ï¼ŒACIDåŸåˆ™ä¸åŒ…æ‹¬ï¼Ÿ", options: ["A. åŸå­æ€§", "B. ä¸€è‡´æ€§", "C. éš”ç¦»æ€§", "D. å¯æ‰©å±•æ€§"], correctIndex: 3 },
];

/**
 * Phase 2: Generates exam paper with 3 questions based on course and difficulty.
 */
export const generateExamPaper = async (
    config: LLMConfig,
    courseName: string,
    difficulty: number
): Promise<ExamQuestion[]> => {
    // Offline fallback
    if (!config.apiKey || config.apiKey.trim() === '') {
        console.log('[Exam] No API key, using mock questions');
        return MOCK_EXAM_QUESTIONS;
    }

    const userPrompt = `è¯¾ç¨‹: ${courseName}\néš¾åº¦: ${difficulty}/5\n\nè¯·ç”Ÿæˆ3é“å•é€‰é¢˜ã€‚`;

    try {
        const response = await withTimeout(callLLM(config, EXAM_SYSTEM_PROMPT, userPrompt), 20000);

        let jsonStr = response.trim();
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/);
        if (jsonMatch) jsonStr = jsonMatch[1].trim();

        const jsonStart = jsonStr.indexOf('{');
        const jsonEnd = jsonStr.lastIndexOf('}');
        if (jsonStart !== -1 && jsonEnd !== -1) {
            jsonStr = jsonStr.substring(jsonStart, jsonEnd + 1);
        }

        const data = JSON.parse(jsonStr);
        if (!Array.isArray(data.questions) || data.questions.length === 0) {
            return MOCK_EXAM_QUESTIONS;
        }

        return data.questions.map((q: any) => ({
            text: q.text || 'æœªçŸ¥é¢˜ç›®',
            options: Array.isArray(q.options) ? q.options : ['A', 'B', 'C', 'D'],
            correctIndex: typeof q.correctIndex === 'number' ? q.correctIndex : 0
        }));
    } catch (error) {
        console.error('[Exam] Question generation failed:', error);
        return MOCK_EXAM_QUESTIONS;
    }
};

const GRADING_SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä½æ‰¹å·çš„æ•™æˆã€‚æ ¹æ®å­¦ç”Ÿçš„ç­”é¢˜æƒ…å†µç»™å‡ºè¯„ä»·ã€‚

è¾“å…¥: è¯¾ç¨‹åï¼Œå­¦ç”Ÿç­”æ¡ˆï¼Œæ­£ç¡®ç­”æ¡ˆ
è¾“å‡ºæ ¼å¼ (Strict JSON only, no markdown):
{
  "score": 0-100,
  "gpaModifier": 0.0-4.0,
  "comment": "æ•™æˆçš„ä¸€å¥è¯è¯„è¯­ï¼Œå¸¦ç‚¹ä¸ªæ€§"
}`;

/**
 * Phase 2: Grades exam with AI-generated comment.
 */
export const gradeExamPaper = async (
    config: LLMConfig,
    playerAnswers: number[],
    correctAnswers: number[],
    courseName: string
): Promise<ExamResult> => {
    // Calculate base score
    let correct = 0;
    for (let i = 0; i < playerAnswers.length; i++) {
        if (playerAnswers[i] === correctAnswers[i]) correct++;
    }
    const baseScore = Math.round((correct / correctAnswers.length) * 100);
    const baseGPA = (correct / correctAnswers.length) * 4.0;

    // Offline fallback
    if (!config.apiKey || config.apiKey.trim() === '') {
        const comments = [
            baseScore >= 80 ? 'ä¸é”™ï¼Œç»§ç»­ä¿æŒï¼' : baseScore >= 60 ? 'åŠæ ¼äº†ï¼Œä½†è¿˜éœ€åŠªåŠ›ã€‚' : 'è¿™æˆç»©...ä¸‹æ¬¡åŠ æ²¹å§ã€‚',
        ];
        return {
            score: baseScore,
            gpaModifier: Number(baseGPA.toFixed(2)),
            comment: comments[0]
        };
    }

    const userPrompt = `è¯¾ç¨‹: ${courseName}
å­¦ç”Ÿç­”æ¡ˆ: ${JSON.stringify(playerAnswers)}
æ­£ç¡®ç­”æ¡ˆ: ${JSON.stringify(correctAnswers)}
æ­£ç¡®é¢˜æ•°: ${correct}/${correctAnswers.length}

è¯·ç»™å‡ºè¯„åˆ†ã€‚`;

    try {
        const response = await withTimeout(callLLM(config, GRADING_SYSTEM_PROMPT, userPrompt), 15000);

        let jsonStr = response.trim();
        const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/);
        if (jsonMatch) jsonStr = jsonMatch[1].trim();

        const jsonStart = jsonStr.indexOf('{');
        const jsonEnd = jsonStr.lastIndexOf('}');
        if (jsonStart !== -1 && jsonEnd !== -1) {
            jsonStr = jsonStr.substring(jsonStart, jsonEnd + 1);
        }

        const data = JSON.parse(jsonStr);
        return {
            score: typeof data.score === 'number' ? data.score : baseScore,
            gpaModifier: typeof data.gpaModifier === 'number' ? Math.min(4.0, Math.max(0, data.gpaModifier)) : baseGPA,
            comment: data.comment || 'è€ƒè¯•ç»“æŸã€‚'
        };
    } catch (error) {
        console.error('[Exam] Grading failed:', error);
        return {
            score: baseScore,
            gpaModifier: Number(baseGPA.toFixed(2)),
            comment: baseScore >= 60 ? 'è€ƒè¯•é€šè¿‡äº†ã€‚' : 'æŒ‚ç§‘äº†ï¼Œéœ€è¦è¡¥è€ƒã€‚'
        };
    }
};

// ============ Phase 4: Autobiography Generation ============

/**
 * Phase 4: Generates personalized autobiography based on 4 years of university life.
 * Style adjusts based on IQ (rational) and EQ (emotional).
 */
export const generateAutobiography = async (
    config: LLMConfig,
    student: StudentState
): Promise<string> => {
    const { attributes, academic, npcs, certificates, achievements, eventHistory, flags, money } = student;

    // Determine writing style based on attributes
    let styleInstruction = '';
    if (attributes.iq >= 80 && attributes.eq < 60) {
        styleInstruction = 'æ–‡é£ç†æ€§ã€é€»è¾‘æ¸…æ™°ï¼Œåƒä¸€ç¯‡å­¦æœ¯è®ºæ–‡æ‘˜è¦ã€‚';
    } else if (attributes.eq >= 80 && attributes.iq < 60) {
        styleInstruction = 'æ–‡é£æ„Ÿæ€§ã€æƒ…æ„Ÿä¸°å¯Œï¼Œå……æ»¡ç»†è…»çš„æƒ…æ„Ÿæå†™ã€‚';
    } else if (attributes.iq >= 70 && attributes.eq >= 70) {
        styleInstruction = 'æ–‡é£å¹³è¡¡ï¼Œæ—¢æœ‰ç†æ€§æ€è€ƒä¹Ÿæœ‰æƒ…æ„Ÿè¡¨è¾¾ï¼Œå¨“å¨“é“æ¥ã€‚';
    } else {
        styleInstruction = 'æ–‡é£æœ´å®ã€çœŸè¯šï¼Œåƒå’Œæœ‹å‹èŠå¤©ä¸€æ ·è‡ªç„¶ã€‚';
    }

    // Gather important NPCs
    const topFriends = npcs
        .filter(n => n.relationshipScore > 50)
        .slice(0, 3)
        .map(n => `${n.name}(${n.role})`);

    // Gather long-term memories
    const allMemories = npcs
        .flatMap(n => n.longTermMemories || [])
        .slice(0, 5);

    // Recent significant events
    const significantEvents = eventHistory
        .slice(-10)
        .map(e => e.title)
        .filter(Boolean)
        .join('ã€');

    const systemPrompt = `ä½ æ˜¯ä¸€ä½ä¼ è®°ä½œå®¶ï¼Œéœ€è¦ä¸ºä¸€ä½åˆšæ¯•ä¸šçš„å¤§å­¦ç”Ÿæ’°å†™ä¸€ç¯‡å›å¿†å½•ã€‚
è¦æ±‚ï¼š
1. ç¬¬ä¸€äººç§°è§†è§’
2. 800å­—å·¦å³
3. ${styleInstruction}
4. èå…¥æä¾›çš„è®°å¿†ç‰‡æ®µå’Œå¥½å‹ä¿¡æ¯
5. ç»“å°¾å¯„è¯­æœªæ¥

è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ¼å¼æ ‡è®°ã€‚`;

    const userPrompt = `æˆ‘çš„å¤§å­¦æ¡£æ¡ˆï¼š
- å§“åï¼š${student.name}
- å­¦æ ¡ï¼š${academic.universityName}
- ä¸“ä¸šï¼š${academic.major.name}
- æœ€ç»ˆGPAï¼š${academic.gpa.toFixed(2)}
- å±æ€§ï¼šæ™ºåŠ›${attributes.iq}ã€æƒ…å•†${attributes.eq}ã€é­…åŠ›${attributes.charm}
- æ¯•ä¸šæ—¶å­˜æ¬¾ï¼šÂ¥${money}
- è¯ä¹¦ï¼š${certificates.length > 0 ? certificates.join('ã€') : 'æ— '}
- æˆå°±ï¼š${achievements.length > 0 ? achievements.join('ã€') : 'æ— '}
- é‡è¦å¥½å‹ï¼š${topFriends.length > 0 ? topFriends.join('ã€') : 'ç‹¬æ¥ç‹¬å¾€'}
- æ‹çˆ±çŠ¶æ€ï¼š${flags.isDating ? 'æœ‰å¯¹è±¡' : 'å•èº«'}
- å°è±¡æ·±åˆ»çš„äº‹ï¼š${significantEvents || 'å¹³æ·¡çš„æ—¥å­'}
- å…³äºæˆ‘çš„è®°å¿†ç¢ç‰‡ï¼š${allMemories.length > 0 ? allMemories.join('ï¼›') : 'æ™®é€šçš„å¤§å­¦ç”Ÿæ´»'}

è¯·æ’°å†™æˆ‘çš„å¤§å­¦å›å¿†å½•ã€‚`;

    const mockBio = `å››å¹´å‰ï¼Œæˆ‘æ€€ç€å¿å¿‘çš„å¿ƒæƒ…è¸å…¥${academic.universityName}çš„æ ¡é—¨ã€‚é‚£æ—¶çš„æˆ‘ï¼Œå¯¹æœªæ¥å……æ»¡æ†§æ†¬ï¼Œå´ä¹Ÿä¸çŸ¥é“å‰æ–¹çš„é“è·¯ä¼šå¦‚ä½•å±•å¼€ã€‚

${academic.major.name}ä¸“ä¸šçš„å­¦ä¹ ï¼Œè®©æˆ‘ä½“ä¼šåˆ°äº†çŸ¥è¯†çš„åŠ›é‡ã€‚æ¯ä¸€æ¬¡è€ƒè¯•å‰çš„ç„¦è™‘ï¼Œæ¯ä¸€æ¬¡è®ºæ–‡æˆªæ­¢æ—¥æœŸçš„é€šå®µï¼Œéƒ½æˆäº†ç°åœ¨çœ‹æ¥çè´µçš„è®°å¿†ã€‚æœ€ç»ˆï¼Œ${academic.gpa.toFixed(2)}çš„GPAè™½ç„¶${academic.gpa >= 3.0 ? 'ç®—ä¸ä¸ŠæƒŠè‰³ï¼Œä½†ä¹Ÿè®©æˆ‘é—®å¿ƒæ— æ„§' : 'æœ‰äº›é—æ†¾ï¼Œä½†æˆ‘çŸ¥é“æˆ‘å·²ç»å°½åŠ›äº†'}ã€‚

${topFriends.length > 0 ? `åœ¨è¿™é‡Œï¼Œæˆ‘é‡åˆ°äº†${topFriends.join('ã€')}ã€‚ä»–ä»¬æ˜¯æˆ‘å¤§å­¦ç”Ÿæ´»ä¸­æœ€é‡è¦çš„äººï¼Œé™ªæˆ‘èµ°è¿‡äº†æ— æ•°ä¸ªæ—¥æ—¥å¤œå¤œã€‚` : 'è™½ç„¶æœ‹å‹ä¸ç®—å¤šï¼Œä½†ç‹¬å¤„çš„æ—¶å…‰è®©æˆ‘å­¦ä¼šäº†ä¸è‡ªå·±å¯¹è¯ã€‚'}

${flags.isDating ? 'æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘åœ¨è¿™é‡Œé‡åˆ°äº†ç”Ÿå‘½ä¸­é‡è¦çš„äººï¼Œtaè®©æˆ‘çš„å¤§å­¦ç”Ÿæ´»å˜å¾—å®Œæ•´ã€‚' : ''}

ç«™åœ¨æ¯•ä¸šå…¸ç¤¼çš„èˆå°ä¸Šï¼Œæˆ‘çŸ¥é“ï¼Œè¿™ä¸æ˜¯ç»“æŸï¼Œè€Œæ˜¯æ–°çš„å¼€å§‹ã€‚æ— è®ºæœªæ¥å¦‚ä½•ï¼Œè¿™å››å¹´éƒ½å°†æ˜¯æˆ‘äººç”Ÿä¸­æœ€ç¾å¥½çš„æ—¶å…‰ä¹‹ä¸€ã€‚

å†è§äº†ï¼Œæˆ‘çš„å¤§å­¦ã€‚ä½ å¥½ï¼Œæ–°çš„äººç”Ÿã€‚`;

    // Offline fallback
    if (!config.apiKey || config.apiKey.trim() === '') {
        return mockBio;
    }

    try {
        const response = await withTimeout(callLLM(config, systemPrompt, userPrompt), 45000);
        return response.trim() || mockBio;
    } catch (error) {
        console.error('[Autobiography] Generation failed:', error);
        return mockBio;
    }
};
